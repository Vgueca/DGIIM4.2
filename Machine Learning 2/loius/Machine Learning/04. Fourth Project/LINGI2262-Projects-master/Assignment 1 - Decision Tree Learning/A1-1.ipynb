{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quality-munich",
   "metadata": {},
   "source": [
    "Question 1 : ID3 attribute selection\n",
    "---\n",
    "\n",
    "Suppose you have the following training set with four boolean attributes $x_1$, $x_2$, $x_3$ and $x_4$ and a boolean output $y$.\n",
    "\n",
    "![](./imgs/q1_2020.png)\n",
    "\n",
    "What is the tree learned by **ID3** from this training set? You should be able to construct it from your general understanding of this algorithm without going into all the details of computing explicitly every step of this algorithm.\n",
    "\n",
    "### Answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expired-vancouver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "feature_names = [\"x1\", \"x2\", \"x3\", \"x4\"]\n",
    "X = [[0, 0, 0, 0],\n",
    "    [0, 0, 1, 1],\n",
    "    [0, 1, 0, 1],\n",
    "    [0, 1, 1, 1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\") \n",
    "clf.fit(X, y)\n",
    "\n",
    "export_graphviz(clf, \n",
    "                feature_names=feature_names,\n",
    "                filled=True,\n",
    "                out_file=\"A11Q1-decision_tree.dot\")\n",
    "\n",
    "!dot -Tpng A11Q1-decision_tree.dot -o A11Q1-decision_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-agreement",
   "metadata": {},
   "source": [
    "![](./A11Q1-decision_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-fraction",
   "metadata": {},
   "source": [
    "Question 2: ID3 attribute selection (continued) \n",
    "---\n",
    "\n",
    "Is there another binary decision tree which would perfectly classify the same training examples and would not be as deep as the one proposed by ID3?\n",
    "\n",
    "- [ ] Yes, since ID3 never finds the minimum-depth tree, on any training set, because it is a greedy algorithm (the attribute selection at a node is done without regard to the selection at children nodes).\n",
    "\n",
    "- [x] Yes, for this example, there exists a smaller tree that perfectly classify all training examples. This tree is not found by ID3 since it is a greedy algorithm (the attribute selection at a node is done without regard to the selection at children nodes).\n",
    "\n",
    "- [ ] No, ID3 found the minimum-depth three for this training set, but it is a matter of chance. In general, ID3 does not always find the minimum-depth tree.\n",
    "\n",
    "- [ ] No, ID3 always finds the minimum-depth tree because it maximizes the information gain at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-senior",
   "metadata": {},
   "source": [
    "Question 3: Information gain \n",
    "---\n",
    "\n",
    "Suppose you have a training set with 12 positive and 12 negative examples.\n",
    "\n",
    "Compute the information gain for the two following splits, performed at the root of the tree:\n",
    "- $[10+, 2-]$(left) $[2+, 10-]$(right)\n",
    "- $[12+, 7-]$(left) $[0+, 5-]$(right)\n",
    "\n",
    "Give your answer in the following format: IG_first, IG_second\n",
    "\n",
    "### Answer :\n",
    "\n",
    "```0.349978, 0.24835```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-amount",
   "metadata": {},
   "source": [
    " Question 4: Information gain (continued) \n",
    " ---\n",
    " \n",
    "Based on your answer in the previous question, which split will be chosen by ID3?\n",
    "\n",
    "Beware: you will only receive credit for this question if you answered the previous one correctly.\n",
    "\n",
    "- [x] $[10+, 2-]$(left) $[2+, 10-]$(right)\n",
    "- [ ] $[12+, 7-]$(left) $[0+, 5-]$(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-license",
   "metadata": {},
   "source": [
    "Question 5: Information gain (continued) \n",
    "---\n",
    "\n",
    "Suppose now that the mistakes on the positive examples are about 10 times as costly as mistakes to the negative. One way of dealing with such a cost imbalance is to replicate the positive examples 10 times each. The splits become:\n",
    "\n",
    "- $[100+, 2-]$(left) $[20+, 10-]$(right)\n",
    "- $[120+, 7-]$(left) $[0+, 5-]$(right)\n",
    "\n",
    "What is their information gain?\n",
    "\n",
    "Give your answer in the following format: IG_first, IG_second\n",
    "\n",
    "### Answer :\n",
    "\n",
    "```0.123204,0.143402```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-constitution",
   "metadata": {},
   "source": [
    "Question 6: Information gain (continued) \n",
    "---\n",
    "\n",
    "Consequently, which split will be performed by ID3?\n",
    "\n",
    "Beware: you will only receive credit for this question if you answered the previous one correctly.\n",
    "\n",
    "- [ ] $[100+, 2-]$(left) $[20+, 10-]$(right)\n",
    "- [x] $[120+, 7-]$(left) $[0+, 5-]$(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-cooking",
   "metadata": {},
   "source": [
    "Question 7: Information gain (continued) \n",
    "---\n",
    "\n",
    "What do you conclude from the previous calculations in terms of splitting criteria and class cost imbalance ? Do you see a numerically equivalent way to decide which is the best split in the previous question without replicating the positive examples?\n",
    "\n",
    "Select all valid sentences\n",
    "\n",
    "- [x] Instead of replicating the positive examples, we can use a weighted proportion in the information gain computation. \n",
    "$p_{\\oplus}=\\frac{n_{\\oplus} \\cdot c_{\\oplus}}{n_{\\oplus} \\cdot c_{\\oplus}+n_{\\ominus} \\cdot c_{\\ominus}} \\quad p_{\\ominus}=\\frac{n_{\\ominus} \\cdot c_{\\ominus}}{n_{\\oplus} \\cdot c_{\\oplus}+n_{\\ominus} \\cdot c_{\\ominus}}$\n",
    "with $c_{\\oplus} = 10$ and $c_{\\ominus} = 1$\n",
    "\n",
    "- [ ] Instead of replicating the positive examples, we can use a weighted proportion in the information gain computation. \n",
    "$p_{\\oplus}=\\frac{n_{\\oplus} \\cdot c_{\\oplus}}{n_{\\oplus} \\cdot c_{\\oplus}+n_{\\ominus} \\cdot c_{\\ominus}} \\quad p_{\\ominus}=\\frac{n_{\\ominus} \\cdot c_{\\ominus}}{n_{\\oplus} \\cdot c_{\\oplus}+n_{\\ominus} \\cdot c_{\\ominus}}$\n",
    "with $c_{\\oplus} = 1$ and $c_{\\ominus} = 10$\n",
    "\n",
    "- [ ] Information gain in insensitive to class cost imbalance.\n",
    "\n",
    "- [x] Information gain is sensitive to class cost imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-baghdad",
   "metadata": {},
   "source": [
    "Question 8: Search space \n",
    "---\n",
    "\n",
    "What is the total number of complete decision trees with c classes, d attributes, each having k possible values (including syntactically distinct trees possibly describing the same model)? The previous figure gives the size of the complete search space of the ID3 algorithm. What is the actual number of different trees considered in ID3 (without pruning)?\n",
    "\n",
    "\n",
    "### Answer :\n",
    "\n",
    "Full search space size : \n",
    "\n",
    "$$c^{k^{d}} \\cdot \\prod_{i=0}^{d-1} k^{i}(d-i)$$\n",
    "\n",
    "ID3 search space size:\n",
    "\n",
    "$$\\sum_{i=0}^{d-1} k^{i}(d-i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-scope",
   "metadata": {},
   "source": [
    "Question 9: C4.5 \n",
    "---\n",
    "\n",
    "Consider a classification problem based only on 2 continuous attributes (the instance space is the plane ‚Ñù$^2$\n",
    "\n",
    "). C4.5 incorporates these attributes by defining threshold based boolean attributes. In the induced tree, each node corresponds to a particular decision boundary splitting the examples into two regions. What are the shape (in the instance space) of the decision boundaries learned by C4.5? Into how many regions is the instance space divided before pruning? Does it depend on the attribute values of the training examples? Does it depend on the number of classes?\n",
    "\n",
    "Select all valid sentences\n",
    "\n",
    "- [ ]  The number of classes influences the shape (e.g. circle shape, rectange shape, ...) of the regions.\n",
    "- [x] The instance space will be divided into four regions, as each attribute will be used to split the space in two.\n",
    "- [x] The number of regions does not depend on the attribute values of the training examples.\n",
    "- [x] The decision boundaries are parallel to the axes. Thus, the instance space is split into rectangles.\n",
    "- [?] The number of regions depends on the number of classes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lingi2262",
   "language": "python",
   "name": "lingi2262"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
