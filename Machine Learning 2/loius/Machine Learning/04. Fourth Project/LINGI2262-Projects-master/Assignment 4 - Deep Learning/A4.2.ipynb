{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e69bb01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Because it's a tiny dataset, we hide GPUs to use the CPU, it will be faster\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "MODEL_PATH = os.path.join(os.getcwd(), \"models\")\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "\n",
    "((x_train, y_train), (x_test, y_test)) = keras.datasets.fashion_mnist.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341b13c5",
   "metadata": {},
   "source": [
    "Question 1 :  A first linear neural network \n",
    "---\n",
    "\n",
    "For this question, we ask you to build and to train a neural network with the following specifications:\n",
    "\n",
    "- The network contains 2 layers:\n",
    "    - A flatten layer which flattens the $28 \\times 28$ 2D images into 1D vectors of size $784$. This layer has no parameter to train, it just reshapes the input data.\n",
    "    - A dense output layer with a softmax activation function such that it can predict the target categorical (=class) variable. The kernel and bias are initializers set to *RandomNormal*.\n",
    "- The network loss is the categorical cross entropy loss.\n",
    "- The network optimizer is the Adam optimizer (an optimized version of the gradient descent procedure) with a learning rate of $10^{-5}$\n",
    "\n",
    "We are here essentially training 10 linear models and then applying a softmax on them. This is **not yet** a deep neural network.\n",
    "\n",
    "Implement your neural network in the variable *model*. Just define and compile the network, don't fit it on the training data (your submission will likely time out if you do!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38edd234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"linear\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(10, kernel_initializer=\"random_normal\", bias_initializer=\"random_normal\", activation=\"softmax\"),\n",
    "], name=\"linear\")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf7413c",
   "metadata": {},
   "source": [
    "Question 2 : A first linear neural network: model fitting \n",
    "---\n",
    "\n",
    "Fit your model from question 1 on the train data with a batch size of $32$. Run $100$ epochs to fit your model.\n",
    "\n",
    "Once your neural network is fitted, save it in a *.model* file using the [save](https://www.tensorflow.org/guide/keras/save_and_serialize) function of Keras with *save_format='h5'* and upload it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea02e0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 103.6690 - accuracy: 0.1771 - val_loss: 33.7150 - val_accuracy: 0.4877\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 30.5127 - accuracy: 0.5240 - val_loss: 24.6446 - val_accuracy: 0.5889\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 23.2020 - accuracy: 0.6055 - val_loss: 20.6909 - val_accuracy: 0.6365\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 19.7961 - accuracy: 0.6441 - val_loss: 18.1776 - val_accuracy: 0.6610\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 17.7093 - accuracy: 0.6666 - val_loss: 16.5538 - val_accuracy: 0.6786\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 15.8994 - accuracy: 0.6870 - val_loss: 15.3571 - val_accuracy: 0.6953\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 14.6782 - accuracy: 0.7015 - val_loss: 14.4035 - val_accuracy: 0.7026\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 13.8134 - accuracy: 0.7133 - val_loss: 13.6346 - val_accuracy: 0.7167\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 12.9576 - accuracy: 0.7204 - val_loss: 13.0735 - val_accuracy: 0.7214\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 12.4077 - accuracy: 0.7309 - val_loss: 12.5230 - val_accuracy: 0.7288\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 11.8222 - accuracy: 0.7322 - val_loss: 12.1442 - val_accuracy: 0.7348\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 11.2968 - accuracy: 0.7404 - val_loss: 11.6403 - val_accuracy: 0.7389\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 10.8458 - accuracy: 0.7459 - val_loss: 11.3352 - val_accuracy: 0.7434\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 10.5925 - accuracy: 0.7497 - val_loss: 11.0451 - val_accuracy: 0.7442\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 10.0841 - accuracy: 0.7536 - val_loss: 10.6964 - val_accuracy: 0.7481\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 9.9143 - accuracy: 0.7601 - val_loss: 10.4657 - val_accuracy: 0.7489\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 9.6036 - accuracy: 0.7623 - val_loss: 10.2032 - val_accuracy: 0.7536\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 9.5379 - accuracy: 0.7634 - val_loss: 9.9923 - val_accuracy: 0.7546\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 9.1307 - accuracy: 0.7675 - val_loss: 9.8201 - val_accuracy: 0.7552\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 8.9882 - accuracy: 0.7672 - val_loss: 9.6973 - val_accuracy: 0.7568\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 8.8236 - accuracy: 0.7720 - val_loss: 9.4265 - val_accuracy: 0.7593\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 8.4879 - accuracy: 0.7712 - val_loss: 9.2456 - val_accuracy: 0.7593\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 8.4546 - accuracy: 0.7736 - val_loss: 9.1150 - val_accuracy: 0.7622\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 8.2876 - accuracy: 0.7754 - val_loss: 8.9567 - val_accuracy: 0.7632\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 8.1336 - accuracy: 0.7760 - val_loss: 8.8269 - val_accuracy: 0.7632\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 8.0724 - accuracy: 0.7778 - val_loss: 8.8045 - val_accuracy: 0.7630\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 7.6830 - accuracy: 0.7831 - val_loss: 8.5685 - val_accuracy: 0.7676\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 7.6480 - accuracy: 0.7823 - val_loss: 8.4841 - val_accuracy: 0.7688\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 7.4960 - accuracy: 0.7846 - val_loss: 8.3646 - val_accuracy: 0.7669\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 7.4578 - accuracy: 0.7824 - val_loss: 8.2586 - val_accuracy: 0.7705\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 7.2072 - accuracy: 0.7853 - val_loss: 8.1389 - val_accuracy: 0.7706\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 7.1212 - accuracy: 0.7854 - val_loss: 8.0568 - val_accuracy: 0.7712\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 7.1420 - accuracy: 0.7841 - val_loss: 7.9484 - val_accuracy: 0.7717\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.8886 - accuracy: 0.7886 - val_loss: 7.8683 - val_accuracy: 0.7725\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.9044 - accuracy: 0.7871 - val_loss: 7.7672 - val_accuracy: 0.7738\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.9325 - accuracy: 0.7876 - val_loss: 7.6918 - val_accuracy: 0.7734\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.7271 - accuracy: 0.7904 - val_loss: 7.6961 - val_accuracy: 0.7754\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.6150 - accuracy: 0.7935 - val_loss: 7.5819 - val_accuracy: 0.7748\n",
      "Epoch 39/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.5884 - accuracy: 0.7880 - val_loss: 7.4848 - val_accuracy: 0.7746\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.4626 - accuracy: 0.7930 - val_loss: 7.3912 - val_accuracy: 0.7767\n",
      "Epoch 41/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.2729 - accuracy: 0.7901 - val_loss: 7.3320 - val_accuracy: 0.7764\n",
      "Epoch 42/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.1741 - accuracy: 0.7965 - val_loss: 7.2726 - val_accuracy: 0.7790\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.2338 - accuracy: 0.7927 - val_loss: 7.1987 - val_accuracy: 0.7770\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.1348 - accuracy: 0.7944 - val_loss: 7.2043 - val_accuracy: 0.7776\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.0129 - accuracy: 0.7954 - val_loss: 7.0736 - val_accuracy: 0.7806\n",
      "Epoch 46/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 6.0500 - accuracy: 0.7940 - val_loss: 7.0353 - val_accuracy: 0.7787\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.8312 - accuracy: 0.7955 - val_loss: 6.9364 - val_accuracy: 0.7797\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.8580 - accuracy: 0.7977 - val_loss: 6.9337 - val_accuracy: 0.7780\n",
      "Epoch 49/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.8697 - accuracy: 0.7956 - val_loss: 6.8454 - val_accuracy: 0.7806\n",
      "Epoch 50/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.7537 - accuracy: 0.7971 - val_loss: 6.7805 - val_accuracy: 0.7813\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.6680 - accuracy: 0.7964 - val_loss: 6.7481 - val_accuracy: 0.7794\n",
      "Epoch 52/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.5806 - accuracy: 0.8011 - val_loss: 6.7326 - val_accuracy: 0.7780\n",
      "Epoch 53/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.5957 - accuracy: 0.7966 - val_loss: 6.6226 - val_accuracy: 0.7810\n",
      "Epoch 54/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.5952 - accuracy: 0.7969 - val_loss: 6.5852 - val_accuracy: 0.7804\n",
      "Epoch 55/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.4427 - accuracy: 0.8000 - val_loss: 6.5575 - val_accuracy: 0.7798\n",
      "Epoch 56/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.3636 - accuracy: 0.8008 - val_loss: 6.4911 - val_accuracy: 0.7827\n",
      "Epoch 57/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.3284 - accuracy: 0.7993 - val_loss: 6.4248 - val_accuracy: 0.7815\n",
      "Epoch 58/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.2844 - accuracy: 0.8014 - val_loss: 6.4077 - val_accuracy: 0.7859\n",
      "Epoch 59/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.1232 - accuracy: 0.8007 - val_loss: 6.3292 - val_accuracy: 0.7823\n",
      "Epoch 60/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.1848 - accuracy: 0.7992 - val_loss: 6.2932 - val_accuracy: 0.7840\n",
      "Epoch 61/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.0434 - accuracy: 0.8054 - val_loss: 6.3278 - val_accuracy: 0.7854\n",
      "Epoch 62/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.1346 - accuracy: 0.8024 - val_loss: 6.2075 - val_accuracy: 0.7841\n",
      "Epoch 63/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.0974 - accuracy: 0.7995 - val_loss: 6.1997 - val_accuracy: 0.7845\n",
      "Epoch 64/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 5.0524 - accuracy: 0.8011 - val_loss: 6.1789 - val_accuracy: 0.7805\n",
      "Epoch 65/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.9821 - accuracy: 0.8043 - val_loss: 6.1273 - val_accuracy: 0.7830\n",
      "Epoch 66/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.8468 - accuracy: 0.8049 - val_loss: 6.1116 - val_accuracy: 0.7814\n",
      "Epoch 67/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.8968 - accuracy: 0.8042 - val_loss: 6.0487 - val_accuracy: 0.7847\n",
      "Epoch 68/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.7923 - accuracy: 0.8025 - val_loss: 5.9748 - val_accuracy: 0.7830\n",
      "Epoch 69/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.7726 - accuracy: 0.8051 - val_loss: 5.9554 - val_accuracy: 0.7825\n",
      "Epoch 70/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.7459 - accuracy: 0.8022 - val_loss: 5.9125 - val_accuracy: 0.7839\n",
      "Epoch 71/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.6785 - accuracy: 0.8066 - val_loss: 5.8617 - val_accuracy: 0.7861\n",
      "Epoch 72/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.7002 - accuracy: 0.8056 - val_loss: 5.8493 - val_accuracy: 0.7847\n",
      "Epoch 73/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.5908 - accuracy: 0.8099 - val_loss: 5.7885 - val_accuracy: 0.7845\n",
      "Epoch 74/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.5774 - accuracy: 0.8083 - val_loss: 5.7491 - val_accuracy: 0.7857\n",
      "Epoch 75/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.4442 - accuracy: 0.8087 - val_loss: 5.7515 - val_accuracy: 0.7841\n",
      "Epoch 76/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.4115 - accuracy: 0.8090 - val_loss: 5.7033 - val_accuracy: 0.7861\n",
      "Epoch 77/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.4065 - accuracy: 0.8091 - val_loss: 5.7047 - val_accuracy: 0.7857\n",
      "Epoch 78/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.4679 - accuracy: 0.8056 - val_loss: 5.6202 - val_accuracy: 0.7862\n",
      "Epoch 79/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.4142 - accuracy: 0.8061 - val_loss: 5.5807 - val_accuracy: 0.7857\n",
      "Epoch 80/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.3394 - accuracy: 0.8102 - val_loss: 5.5830 - val_accuracy: 0.7867\n",
      "Epoch 81/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.3043 - accuracy: 0.8077 - val_loss: 5.5617 - val_accuracy: 0.7861\n",
      "Epoch 82/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.3357 - accuracy: 0.8094 - val_loss: 5.4762 - val_accuracy: 0.7859\n",
      "Epoch 83/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.1823 - accuracy: 0.8070 - val_loss: 5.4505 - val_accuracy: 0.7870\n",
      "Epoch 84/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.2401 - accuracy: 0.8085 - val_loss: 5.4555 - val_accuracy: 0.7867\n",
      "Epoch 85/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.2427 - accuracy: 0.8073 - val_loss: 5.4202 - val_accuracy: 0.7869\n",
      "Epoch 86/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.1482 - accuracy: 0.8075 - val_loss: 5.3585 - val_accuracy: 0.7888\n",
      "Epoch 87/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.2102 - accuracy: 0.8083 - val_loss: 5.3395 - val_accuracy: 0.7894\n",
      "Epoch 88/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.0471 - accuracy: 0.8098 - val_loss: 5.3279 - val_accuracy: 0.7887\n",
      "Epoch 89/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.0750 - accuracy: 0.8079 - val_loss: 5.3085 - val_accuracy: 0.7872\n",
      "Epoch 90/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 4.0779 - accuracy: 0.8102 - val_loss: 5.2484 - val_accuracy: 0.7901\n",
      "Epoch 91/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3.9861 - accuracy: 0.8101 - val_loss: 5.2652 - val_accuracy: 0.7883\n",
      "Epoch 92/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3.9681 - accuracy: 0.8112 - val_loss: 5.2063 - val_accuracy: 0.7893\n",
      "Epoch 93/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3.9085 - accuracy: 0.8108 - val_loss: 5.1814 - val_accuracy: 0.7917\n",
      "Epoch 94/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3.8498 - accuracy: 0.8115 - val_loss: 5.2077 - val_accuracy: 0.7867\n",
      "Epoch 95/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3.8577 - accuracy: 0.8077 - val_loss: 5.1618 - val_accuracy: 0.7884\n",
      "Epoch 96/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3.8336 - accuracy: 0.8089 - val_loss: 5.0965 - val_accuracy: 0.7903\n",
      "Epoch 97/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3.7980 - accuracy: 0.8132 - val_loss: 5.0684 - val_accuracy: 0.7911\n",
      "Epoch 98/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3.8408 - accuracy: 0.8142 - val_loss: 5.0381 - val_accuracy: 0.7912\n",
      "Epoch 99/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3.8674 - accuracy: 0.8110 - val_loss: 5.0294 - val_accuracy: 0.7897\n",
      "Epoch 100/100\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 3.7525 - accuracy: 0.8109 - val_loss: 4.9764 - val_accuracy: 0.7916\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "fit_feedback = model.fit(x_train, y_train, \n",
    "                         validation_data=(x_test, y_test),\n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         epochs=EPOCHS,\n",
    "                         use_multiprocessing=True)\n",
    "model.save(os.path.join(MODEL_PATH, f'{model.name}.model'), save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb3ed44",
   "metadata": {},
   "source": [
    "Question 3 : A first linear neural network: performance \n",
    "---\n",
    "\n",
    "How many trainable parameters are contained in the whole network you just built? What are the measured train and test accuracies of the model you fitted in question 2?\n",
    "\n",
    "Report your answer under the format: *number_param*, *train_acc*, *test_acc* (use a decimal notation for the accuracies, not %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71c8da56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_param, train_acc, test_acc :: 7850, 0.814, 0.792\n"
     ]
    }
   ],
   "source": [
    "# We already had the number of parameters in the model summary\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "history = fit_feedback.history\n",
    "number_param = tf.reduce_sum([reduce(operator.mul, v.shape) for v in model.trainable_variables]).numpy()\n",
    "train_acc = model.evaluate(x_train, y_train)[-1]\n",
    "test_acc = model.evaluate(x_test, y_test)[-1]\n",
    "\n",
    "print(f\"number_param, train_acc, test_acc :: {number_param}, {train_acc:.3f}, {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19306d39",
   "metadata": {},
   "source": [
    "Question 4 : A non-linear network \n",
    "---\n",
    "\n",
    "Build a new model, by adding a layer before the output layer of your neural net from question 1. This layer must be a dense layer with a tanh activation function, and should contain $100$ units. The kernel and bias are initialized to *random_normal*.\n",
    "\n",
    "Use a learning rate of $10^{-5}$.\n",
    "\n",
    "Implement you neural network in the variable *model*. Just define and compile the network, don't fit it on the training data (your submission will likely time out if you do!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75e1e60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"non_linear\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(100, kernel_initializer=\"random_normal\", bias_initializer=\"random_normal\", activation=\"tanh\"),\n",
    "    tf.keras.layers.Dense(10, kernel_initializer=\"random_normal\", bias_initializer=\"random_normal\", activation=\"softmax\"),\n",
    "], name=\"non_linear\")\n",
    "\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d05406",
   "metadata": {},
   "source": [
    "Question 5 : \n",
    "---\n",
    "\n",
    "Fit your model from question 4 on the train data with a batch size of $32$. Run $100$ epochs to fit your model.\n",
    "\n",
    "Once your neural network is fitted, save it in a *.model* file using the [save](https://www.tensorflow.org/guide/keras/save_and_serialize) function of Keras with *save_format='h5'* and upload it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c126aebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1875/1875 [==============================] - 7s 3ms/step - loss: 2.2050 - accuracy: 0.2162 - val_loss: 1.6161 - val_accuracy: 0.5738\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 1.5102 - accuracy: 0.6107 - val_loss: 1.2823 - val_accuracy: 0.6599\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 1.2188 - accuracy: 0.6727 - val_loss: 1.0899 - val_accuracy: 0.6914\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 1.0397 - accuracy: 0.7003 - val_loss: 0.9648 - val_accuracy: 0.7016\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.9226 - accuracy: 0.7193 - val_loss: 0.8791 - val_accuracy: 0.7184\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.8427 - accuracy: 0.7354 - val_loss: 0.8183 - val_accuracy: 0.7293\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.7850 - accuracy: 0.7467 - val_loss: 0.7737 - val_accuracy: 0.7429\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.7384 - accuracy: 0.7585 - val_loss: 0.7369 - val_accuracy: 0.7525\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.7096 - accuracy: 0.7633 - val_loss: 0.7090 - val_accuracy: 0.7588\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6796 - accuracy: 0.7737 - val_loss: 0.6845 - val_accuracy: 0.7625\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6561 - accuracy: 0.7799 - val_loss: 0.6631 - val_accuracy: 0.7686\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6326 - accuracy: 0.7840 - val_loss: 0.6469 - val_accuracy: 0.7714\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6216 - accuracy: 0.7871 - val_loss: 0.6306 - val_accuracy: 0.7799\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.6010 - accuracy: 0.7935 - val_loss: 0.6220 - val_accuracy: 0.7805\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5900 - accuracy: 0.7975 - val_loss: 0.6070 - val_accuracy: 0.7872\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5746 - accuracy: 0.8006 - val_loss: 0.5974 - val_accuracy: 0.7900\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5648 - accuracy: 0.8043 - val_loss: 0.5887 - val_accuracy: 0.7881\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5495 - accuracy: 0.8078 - val_loss: 0.5785 - val_accuracy: 0.7957\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5438 - accuracy: 0.8080 - val_loss: 0.5720 - val_accuracy: 0.7966\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5358 - accuracy: 0.8109 - val_loss: 0.5662 - val_accuracy: 0.7993\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5353 - accuracy: 0.8134 - val_loss: 0.5591 - val_accuracy: 0.7999\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5278 - accuracy: 0.8125 - val_loss: 0.5512 - val_accuracy: 0.8039\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5145 - accuracy: 0.8172 - val_loss: 0.5493 - val_accuracy: 0.8037\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5093 - accuracy: 0.8190 - val_loss: 0.5415 - val_accuracy: 0.8055\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5076 - accuracy: 0.8205 - val_loss: 0.5387 - val_accuracy: 0.8046\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4987 - accuracy: 0.8241 - val_loss: 0.5326 - val_accuracy: 0.8094\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4980 - accuracy: 0.8239 - val_loss: 0.5290 - val_accuracy: 0.8100\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4901 - accuracy: 0.8262 - val_loss: 0.5240 - val_accuracy: 0.8136\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4890 - accuracy: 0.8240 - val_loss: 0.5234 - val_accuracy: 0.8124\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4885 - accuracy: 0.8244 - val_loss: 0.5224 - val_accuracy: 0.8145\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4784 - accuracy: 0.8291 - val_loss: 0.5181 - val_accuracy: 0.8151\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4697 - accuracy: 0.8315 - val_loss: 0.5132 - val_accuracy: 0.8156\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4675 - accuracy: 0.8324 - val_loss: 0.5091 - val_accuracy: 0.8190\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4647 - accuracy: 0.8341 - val_loss: 0.5070 - val_accuracy: 0.8180\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4606 - accuracy: 0.8361 - val_loss: 0.5061 - val_accuracy: 0.8174\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4531 - accuracy: 0.8378 - val_loss: 0.5017 - val_accuracy: 0.8184\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4552 - accuracy: 0.8377 - val_loss: 0.5018 - val_accuracy: 0.8217\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4505 - accuracy: 0.8391 - val_loss: 0.4981 - val_accuracy: 0.8216\n",
      "Epoch 39/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4550 - accuracy: 0.8357 - val_loss: 0.4945 - val_accuracy: 0.8222\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4477 - accuracy: 0.8400 - val_loss: 0.4929 - val_accuracy: 0.8217\n",
      "Epoch 41/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4465 - accuracy: 0.8402 - val_loss: 0.4927 - val_accuracy: 0.8247\n",
      "Epoch 42/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4455 - accuracy: 0.8405 - val_loss: 0.4935 - val_accuracy: 0.8194\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4417 - accuracy: 0.8414 - val_loss: 0.4899 - val_accuracy: 0.8222\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4360 - accuracy: 0.8465 - val_loss: 0.4865 - val_accuracy: 0.8255\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4371 - accuracy: 0.8422 - val_loss: 0.4839 - val_accuracy: 0.8257\n",
      "Epoch 46/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4366 - accuracy: 0.8428 - val_loss: 0.4823 - val_accuracy: 0.8276\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4311 - accuracy: 0.8447 - val_loss: 0.4775 - val_accuracy: 0.8306\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4285 - accuracy: 0.8456 - val_loss: 0.4811 - val_accuracy: 0.8251\n",
      "Epoch 49/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4246 - accuracy: 0.8469 - val_loss: 0.4766 - val_accuracy: 0.8292\n",
      "Epoch 50/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4233 - accuracy: 0.8494 - val_loss: 0.4760 - val_accuracy: 0.8290\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4222 - accuracy: 0.8486 - val_loss: 0.4779 - val_accuracy: 0.8296\n",
      "Epoch 52/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4167 - accuracy: 0.8520 - val_loss: 0.4757 - val_accuracy: 0.8274\n",
      "Epoch 53/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4154 - accuracy: 0.8512 - val_loss: 0.4730 - val_accuracy: 0.8305\n",
      "Epoch 54/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4217 - accuracy: 0.8483 - val_loss: 0.4710 - val_accuracy: 0.8299\n",
      "Epoch 55/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4139 - accuracy: 0.8526 - val_loss: 0.4691 - val_accuracy: 0.8302\n",
      "Epoch 56/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4144 - accuracy: 0.8518 - val_loss: 0.4705 - val_accuracy: 0.8324\n",
      "Epoch 57/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4137 - accuracy: 0.8506 - val_loss: 0.4689 - val_accuracy: 0.8317\n",
      "Epoch 58/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4069 - accuracy: 0.8537 - val_loss: 0.4674 - val_accuracy: 0.8327\n",
      "Epoch 59/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4096 - accuracy: 0.8533 - val_loss: 0.4664 - val_accuracy: 0.8306\n",
      "Epoch 60/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4057 - accuracy: 0.8551 - val_loss: 0.4667 - val_accuracy: 0.8320\n",
      "Epoch 61/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4053 - accuracy: 0.8538 - val_loss: 0.4642 - val_accuracy: 0.8331\n",
      "Epoch 62/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4001 - accuracy: 0.8559 - val_loss: 0.4638 - val_accuracy: 0.8304\n",
      "Epoch 63/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4005 - accuracy: 0.8553 - val_loss: 0.4644 - val_accuracy: 0.8315\n",
      "Epoch 64/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3978 - accuracy: 0.8552 - val_loss: 0.4606 - val_accuracy: 0.8340\n",
      "Epoch 65/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3919 - accuracy: 0.8598 - val_loss: 0.4610 - val_accuracy: 0.8353\n",
      "Epoch 66/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3934 - accuracy: 0.8571 - val_loss: 0.4601 - val_accuracy: 0.8323\n",
      "Epoch 67/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3960 - accuracy: 0.8582 - val_loss: 0.4597 - val_accuracy: 0.8341\n",
      "Epoch 68/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3933 - accuracy: 0.8584 - val_loss: 0.4580 - val_accuracy: 0.8359\n",
      "Epoch 69/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3896 - accuracy: 0.8615 - val_loss: 0.4554 - val_accuracy: 0.8351\n",
      "Epoch 70/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3884 - accuracy: 0.8613 - val_loss: 0.4554 - val_accuracy: 0.8355\n",
      "Epoch 71/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3911 - accuracy: 0.8575 - val_loss: 0.4559 - val_accuracy: 0.8357\n",
      "Epoch 72/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3863 - accuracy: 0.8602 - val_loss: 0.4528 - val_accuracy: 0.8375\n",
      "Epoch 73/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3813 - accuracy: 0.8620 - val_loss: 0.4525 - val_accuracy: 0.8380\n",
      "Epoch 74/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3831 - accuracy: 0.8620 - val_loss: 0.4507 - val_accuracy: 0.8363\n",
      "Epoch 75/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3807 - accuracy: 0.8628 - val_loss: 0.4520 - val_accuracy: 0.8375\n",
      "Epoch 76/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3809 - accuracy: 0.8632 - val_loss: 0.4522 - val_accuracy: 0.8365\n",
      "Epoch 77/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3783 - accuracy: 0.8625 - val_loss: 0.4512 - val_accuracy: 0.8356\n",
      "Epoch 78/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3806 - accuracy: 0.8623 - val_loss: 0.4495 - val_accuracy: 0.8389\n",
      "Epoch 79/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3859 - accuracy: 0.8612 - val_loss: 0.4526 - val_accuracy: 0.8371\n",
      "Epoch 80/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3751 - accuracy: 0.8650 - val_loss: 0.4516 - val_accuracy: 0.8369\n",
      "Epoch 81/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3787 - accuracy: 0.8634 - val_loss: 0.4512 - val_accuracy: 0.8376\n",
      "Epoch 82/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3752 - accuracy: 0.8652 - val_loss: 0.4508 - val_accuracy: 0.8364\n",
      "Epoch 83/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3711 - accuracy: 0.8663 - val_loss: 0.4474 - val_accuracy: 0.8402\n",
      "Epoch 84/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3744 - accuracy: 0.8654 - val_loss: 0.4468 - val_accuracy: 0.8408\n",
      "Epoch 85/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3685 - accuracy: 0.8685 - val_loss: 0.4470 - val_accuracy: 0.8405\n",
      "Epoch 86/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3692 - accuracy: 0.8656 - val_loss: 0.4434 - val_accuracy: 0.8412\n",
      "Epoch 87/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3681 - accuracy: 0.8667 - val_loss: 0.4473 - val_accuracy: 0.8385\n",
      "Epoch 88/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3720 - accuracy: 0.8655 - val_loss: 0.4432 - val_accuracy: 0.8402\n",
      "Epoch 89/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3669 - accuracy: 0.8687 - val_loss: 0.4461 - val_accuracy: 0.8403\n",
      "Epoch 90/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3663 - accuracy: 0.8673 - val_loss: 0.4455 - val_accuracy: 0.8406\n",
      "Epoch 91/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3672 - accuracy: 0.8666 - val_loss: 0.4441 - val_accuracy: 0.8407\n",
      "Epoch 92/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3642 - accuracy: 0.8684 - val_loss: 0.4423 - val_accuracy: 0.8413\n",
      "Epoch 93/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3593 - accuracy: 0.8707 - val_loss: 0.4431 - val_accuracy: 0.8399\n",
      "Epoch 94/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3647 - accuracy: 0.8685 - val_loss: 0.4430 - val_accuracy: 0.8391\n",
      "Epoch 95/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3581 - accuracy: 0.8723 - val_loss: 0.4429 - val_accuracy: 0.8438\n",
      "Epoch 96/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3565 - accuracy: 0.8732 - val_loss: 0.4420 - val_accuracy: 0.8427\n",
      "Epoch 97/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3589 - accuracy: 0.8706 - val_loss: 0.4416 - val_accuracy: 0.8399\n",
      "Epoch 98/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3529 - accuracy: 0.8718 - val_loss: 0.4429 - val_accuracy: 0.8413\n",
      "Epoch 99/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3590 - accuracy: 0.8717 - val_loss: 0.4396 - val_accuracy: 0.8421\n",
      "Epoch 100/100\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.3535 - accuracy: 0.8734 - val_loss: 0.4417 - val_accuracy: 0.8400\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "fit_feedback = model.fit(x_train, y_train, \n",
    "                         validation_data=(x_test, y_test),\n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         epochs=EPOCHS,\n",
    "                         use_multiprocessing=True)\n",
    "model.save(os.path.join(MODEL_PATH, f'{model.name}.model'), save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807452cc",
   "metadata": {},
   "source": [
    "Question 6 : \n",
    "---\n",
    "\n",
    "How many trainable parameters are contained in the whole network you built in question 4? What are the measured train and test accuracies of the model as fitted in question 5?\n",
    "\n",
    "Report your answer under the format: *number_param*, *train_acc*, *test_acc* (use a decimal notation for the accuracies, not %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "351509f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_param, train_acc, test_acc :: 79510, 0.874, 0.840\n"
     ]
    }
   ],
   "source": [
    "# We already had the number of parameters in the model summary\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "history = fit_feedback.history\n",
    "number_param = tf.reduce_sum([reduce(operator.mul, v.shape) for v in model.trainable_variables]).numpy()\n",
    "train_acc = model.evaluate(x_train, y_train)[-1]\n",
    "test_acc = model.evaluate(x_test, y_test)[-1]\n",
    "\n",
    "print(f\"number_param, train_acc, test_acc :: {number_param}, {train_acc:.3f}, {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f7d4e",
   "metadata": {},
   "source": [
    "Question 7 :\n",
    "---\n",
    "\n",
    "Besides a tanh activation funtion, other non-linear functions can be implemented in a hidden layer. Let's consider the ReLU activation: use the **exact** same network as in the previous question, but with ReLU instead of tanh activation in the hidden layer. Which one performs better?\n",
    "\n",
    "Since there is a lot of randomness involved, different training runs for the same network might yield different results. To get more robust results, perform $10$ distinct runs for each model and report the average test accuracies.\n",
    "\n",
    "Train each model during $100$ epochs.\n",
    "\n",
    "Report the mean test accuracy of both networks using the format: *tanh_acc*, *relu_acc* (use a decimal notation, not %)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25aeed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tanh_acc, relu_acc :: 0.841, 0.852\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "from time import time\n",
    "from tqdm import trange\n",
    "\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "N_RUNS = 10\n",
    "\n",
    "def get_accuracies(activation, output):\n",
    "    \n",
    "    def get_non_linear_model():\n",
    "        \"\"\"\n",
    "        Return the global model with the good activation function for the hidden layer\n",
    "        \"\"\"\n",
    "        \n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(100, kernel_initializer=\"random_normal\", bias_initializer=\"random_normal\", activation=activation),\n",
    "            tf.keras.layers.Dense(10, kernel_initializer=\"random_normal\", bias_initializer=\"random_normal\", activation=\"softmax\"),\n",
    "        ], name=\"non_linear\")\n",
    "        \n",
    "        model.compile(\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    mean_acc = .0\n",
    "    mean_elapsed = .0\n",
    "    global_start = time()\n",
    "    for run in range(N_RUNS):\n",
    "        start = time()\n",
    "        model = get_non_linear_model()\n",
    "        fit_feedback = model.fit(x_train, y_train, \n",
    "                                 validation_data=(x_test, y_test),\n",
    "                                 batch_size=BATCH_SIZE, \n",
    "                                 epochs=EPOCHS,\n",
    "                                 use_multiprocessing=True,\n",
    "                                 verbose=0)\n",
    "        acc =  fit_feedback.history[\"val_accuracy\"][-1]\n",
    "        output.append(acc)\n",
    "        \n",
    "        # Just needed to log\n",
    "        elapsed = time()-start\n",
    "        global_elapsed = time()-global_start\n",
    "        \n",
    "        mean_elapsed = ((run)*mean_elapsed + elapsed) / (run+1)\n",
    "        mean_acc = ((run)*mean_acc + acc) / (run+1)\n",
    "        \n",
    "        remaining_time = global_elapsed + (N_RUNS-(run+1)) * mean_elapsed\n",
    "        \n",
    "        print(f\"[{activation}] run {run+1}/{N_RUNS} :: mean accuracy={mean_acc:.3f} :: [{global_elapsed:.2f}s < {remaining_time:.2f}s]\")\n",
    "        \n",
    "\n",
    "# To run faster (while the cpu is not bottlenecked), we will run both model (tanh, relu) in parallel\n",
    "tanh_accuracies = list()\n",
    "relu_accuracies = list()\n",
    "tanh_thread = Thread(target=get_accuracies, kwargs=dict(activation=\"tanh\", output=tanh_accuracies))\n",
    "relu_thread = Thread(target=get_accuracies, kwargs=dict(activation=\"relu\", output=relu_accuracies))\n",
    "\n",
    "tanh_thread.start(); relu_thread.start();\n",
    "tanh_thread.join(); relu_thread.join();\n",
    "\n",
    "tanh_acc = tf.reduce_mean(tanh_accuracies).numpy()\n",
    "relu_acc = tf.reduce_mean(relu_accuracies).numpy()\n",
    "\n",
    "print(f'\\ntanh_acc, relu_acc :: {tanh_acc:.3f}, {relu_acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92724a6",
   "metadata": {},
   "source": [
    "Question 8 : Multiple choice\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f812c5",
   "metadata": {},
   "source": [
    "- [x] In the neural network of Q4, the only introduced non-linearities come from the activation functions.\n",
    "- [x] Each epoch during the learning of the neural network of Q4 takes more time as there are more trainable parameters (as compared to the initial network fitted in Q2).\n",
    "- [ ] The kernel and bias initializers do not influence the final solution. The gradient descent always converges towards the same solution as the minimization problem is convex.\n",
    "- [ ] With a sufficiently small learning rate, the categorical accuracy on the test set is guaranteed to increase after each epoch.\n",
    "- [ ] The ReLU activation function is linear\n",
    "- [ ] The learning rate does not influence a lot the learned neural network, it mainly influences the number of epochs until convergence.\n",
    "- [ ] With a sufficiently small learning rate, the categorical accuracy on the train set is guaranteed to increase after each epoch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingi2262",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
