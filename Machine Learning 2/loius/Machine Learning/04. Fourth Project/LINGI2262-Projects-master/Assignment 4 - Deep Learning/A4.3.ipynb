{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca8f7dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import threading\n",
    "from threading import Thread\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "((X_train, y_train), (X_test, y_test)) = keras.datasets.fashion_mnist.load_data()\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "def config_single_gpu(gpu):\n",
    "    if isinstance(gpu, int):\n",
    "        gpu = tf.config.list_physical_devices(\"GPU\")[gpu]\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    tf.config.experimental.set_visible_devices(gpu, 'GPU')\n",
    "\n",
    "class AttrDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "        \n",
    "class Simulation:\n",
    "    def __init__(self, lr, layers_params):\n",
    "        self.lr = lr\n",
    "        self.layers_params = layers_params\n",
    "    \n",
    "    def get_model(self):\n",
    "        layers = [\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        ]\n",
    "        for params in self.layers_params:\n",
    "            layers.append(\n",
    "                tf.keras.layers.Dense(params.n, kernel_initializer=\"random_normal\", bias_initializer=\"random_normal\", activation=\"tanh\")\n",
    "            )\n",
    "        layers.append(\n",
    "            tf.keras.layers.Dense(10, kernel_initializer=\"random_normal\", bias_initializer=\"random_normal\", activation=\"softmax\")\n",
    "        )\n",
    "        model = tf.keras.models.Sequential(layers)\n",
    "        model.compile(\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            optimizer=tf.keras.optimizers.Adam(lr=self.lr),\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def run(self, train_ds, test_ds, runs, epochs, batch_size, output=[]):\n",
    "        def fit_worker(*args):\n",
    "            model = self.get_model()\n",
    "            fit_feedback = model.fit(train_ds,\n",
    "                                     validation_data=test_ds,\n",
    "                                     batch_size=batch_size, \n",
    "                                     epochs=epochs,\n",
    "                                     use_multiprocessing=True,\n",
    "                                     verbose=0)\n",
    "            acc =  fit_feedback.history[\"val_accuracy\"][-1]\n",
    "            return acc\n",
    "        \n",
    "        with ThreadPool() as p:\n",
    "            acc = p.map(fit_worker, range(runs))\n",
    "            \n",
    "        out = tf.reduce_mean(acc).numpy()\n",
    "        output.append(out)\n",
    "\n",
    "class ThreadSimulations:\n",
    "    def __init__(self, lr, layers_params):\n",
    "        self.lr = lr\n",
    "        self.layers_params = layers_params\n",
    "        self.simulations = [Simulation(lr, lp) for lp in self.layers_params]\n",
    "    \n",
    "    def run(self, train_ds, test_ds, runs, epochs, batch_size, mean=True):\n",
    "        outputs = [[] for _ in self.simulations]\n",
    "        threads = [\n",
    "            Thread(target=sim.run, args=(train_ds, test_ds, runs, epochs, batch_size, out)) for sim, out in zip(self.simulations, outputs)\n",
    "        ]\n",
    "        \n",
    "        for t in threads:\n",
    "            t.start()\n",
    "        \n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        \n",
    "        if mean:\n",
    "            return [tf.reduce_mean(out).numpy() for out in outputs]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17445538",
   "metadata": {},
   "source": [
    "Question 1 : Hidden units\n",
    "---\n",
    "\n",
    "Take again the neural network you defined in question 4 of the previous task (one hidden layer with *tanh* activation, with a learning rate of $10^{-5}$\n",
    "\n",
    "). Let's study the impact of the number of units in the hidden layer.\n",
    "\n",
    "Build a model with 10 units in the hidden layer, one with $100$ units, and another one with $1000$ units. Which one performs best ?\n",
    "\n",
    "Perform $10$ distinct runs (training + testing) for each model and average the results. Use $100$ epochs to fit each model.\n",
    "\n",
    "Don't change anything in your network besides the number of hidden units.\n",
    "\n",
    "Report the mean test accuracies of the three models using the format: *test_acc_10*, *test_acc_100*, *test_acc_1000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9918c705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc_10, test_acc_100, test_acc_1000 ::  0.743, 0.839, 0.867\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 10\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "n_hiddens = [10, 100, 1000]\n",
    "\n",
    "config_single_gpu(1)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).cache().batch(BATCH_SIZE).prefetch(-1)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).cache().batch(BATCH_SIZE).prefetch(-1)\n",
    "\n",
    "layers_params = [[AttrDict(n=n)] for n in n_hiddens]\n",
    "\n",
    "single_layer_simu = ThreadSimulations(1e-5, layers_params)\n",
    "means = single_layer_simu.run(train_ds, test_ds, N_RUNS, N_EPOCHS, BATCH_SIZE)\n",
    "\n",
    "print(f\"test_acc_10, test_acc_100, test_acc_1000 :: \", \", \".join([f\"{v:.3f}\" for v in means]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496f410",
   "metadata": {},
   "source": [
    "Question 2 : Adding layers\n",
    "---\n",
    "\n",
    "We will now investigate whether we can improve the classification accuracy by adding extra layers to the model.\n",
    "\n",
    "Compare (locally, on your machine) the models from question 1 to models having 2 or 3 hidden layers. Try different combinations of layers with 10, 100 or 1000 units (do not use any other number). Which configuration performs best?\n",
    "\n",
    "Don't change anything in your network besides the numbers of layers and of hidden units.\n",
    "\n",
    "Once you have identified a best model (if different configurations perform equally well, pick one), implement it in the variable model. Just define and compile the network, do not fit it on the training data (your submission will likely time out if you do!).\n",
    "\n",
    "Hint: There are many possible configurations, so it would be quite inefficient to perform 10 runs for each one. First get some initial results using only 1 run (but of course consider several epochs of training and, ideally, monitor convergence). Afterwards, you may want to perform additional runs to discriminate the most promissing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16895961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, combinations\n",
    "N_RUNS = 10\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "n_hiddens = [10, 100, 1000]\n",
    "n_layers = [1, 2, 3]\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).cache().batch(BATCH_SIZE).prefetch(-1)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).cache().batch(BATCH_SIZE).prefetch(-1)\n",
    "\n",
    "layers_params = []\n",
    "for n_layer in n_layers:\n",
    "    combi = list(product(n_hiddens, repeat=n_layer))\n",
    "    for params in combi:\n",
    "        layers_params.append(\n",
    "            [AttrDict(n=n) for n in params]\n",
    "        )\n",
    "\n",
    "multi_layer_simu = ThreadSimulations(1e-5, layers_params)\n",
    "means = multi_layer_simu.run(train_ds, test_ds, N_RUNS, N_EPOCHS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a5ef1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params for layers :: \n",
      " 1000 1000\n"
     ]
    }
   ],
   "source": [
    "best_mean_idx = tf.argmax(means).numpy()\n",
    "print(\"Best params for layers :: \", *[a.n for a in layers_params[best_mean_idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef13b2f2",
   "metadata": {},
   "source": [
    "### Here is the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eb7b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              785000    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                10010     \n",
      "=================================================================\n",
      "Total params: 1,796,010\n",
      "Trainable params: 1,796,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(1000, activation='tanh', kernel_initializer='random_normal', bias_initializer='random_normal'),\n",
    "    tf.keras.layers.Dense(1000, activation='tanh', kernel_initializer='random_normal', bias_initializer='random_normal'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax', kernel_initializer='random_normal', bias_initializer='random_normal'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db3d2ac",
   "metadata": {},
   "source": [
    "Question 3 : Adding layers (continued)\n",
    "---\n",
    "\n",
    "Based on the experiments you made for the previous questions, select all valid affirmations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d37fd",
   "metadata": {},
   "source": [
    "- [ ] There are models with multiple hidden layers that significantly outperform (e.g. a difference of > 5%) the best model with one hidden layer.\n",
    "- [ ] For networks with 3 layers, the test accuracy seems to depend mostly on the number of nodes in the 2nd hidden layer.\n",
    "- [ ] Increasing the number of trainable parameters in the model always leads to an increase of test accuracy.\n",
    "- [x] There are no models with multiple hidden layers that significantly outperform (e.g. a difference of > 5%) the best model with one hidden layer.\n",
    "- [ ] The test accurracy does not seem to depend on any number of layers in particular.\n",
    "- [ ] For networks with 3 layers, the test accuracy seems to depend mostly on the number of nodes in the 3rd hidden layer.\n",
    "- [x] For networks with 3 layers, the test accuracy seems to depend mostly on the number of nodes in the 1st hidden layer.\n",
    "- [ ] Compared to a model with one hidden layer, adding extra layers never significantly decrease the performance.\n",
    "- [x] More is not always better: in some cases, the test accuracy significantly decreases when adding extra layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4809d5",
   "metadata": {},
   "source": [
    "Question 4: ReLU activation \n",
    "---\n",
    "\n",
    "Now, let's see if your results change with a different activation function.\n",
    "\n",
    "Re-run some of the models from the previous questions (e.g. the best model you selected, a poorly performing model, and a few in between), but this time using ReLU instead of tanh activation in the hidden layers. Possibly revisit your conclusions by answering the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c412a7a",
   "metadata": {},
   "source": [
    "- [ ] Overall, ReLU activation yields worse results than tanh.\n",
    "- [x] There are no models with multiple hidden layers that significantly outperform (e.g. a difference of > 5%) the best model with one hidden layer.\n",
    "- [ ] More is not always better: in some cases, the test accuracy significantly decreases when adding extra layers.\n",
    "- [ ] There are models with multiple hidden layers that significantly outperform (e.g. a difference of > 5%) the best model with one hidden layer.\n",
    "- [x] Overall, ReLU activation yields better results than tanh.\n",
    "- [ ] Compared to a model with one hidden layer, adding extra layers never significantly decrease the performance.\n",
    "- [ ] Overall, there is no real difference in performance when using ReLU or tanh."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ingi2262",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
